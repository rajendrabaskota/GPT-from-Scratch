{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06bc2746",
   "metadata": {},
   "source": [
    "## **Training BPE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5ae398",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "from tokenizers import ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37d96fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_apostrophe_space(input_string):\n",
    "    pattern = re.compile(r\"(?<=\\w)'\\s(?=[tslvrm])\")\n",
    "    output_string = re.sub(pattern, \"'\", input_string)\n",
    "\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884cfc49",
   "metadata": {},
   "source": [
    "## Loading the DataSet\n",
    "* Add the kaggle dataset **Machine Translation Data Set** by Aadish Joshi to your working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcc757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    text = \"\"\n",
    "\n",
    "    with open(\"/kaggle/input/machine-translation-data-set/enlish_data.txt\", 'r', encoding='utf-8') as f:\n",
    "        a_book = f.read()\n",
    "        \n",
    "    text += a_book\n",
    "    text = fix_apostrophe_space(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d324b15",
   "metadata": {},
   "source": [
    "## **Generating BPE and Storing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f931ad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = [\"/kaggle/input/machine-translation-data-set/enlish_data.txt\"]\n",
    "tokenizer = ByteLevelBPETokenizer()\n",
    "\n",
    "tokenizer.train(files=path, vocab_size=18000, min_frequency=2, special_tokens=[\n",
    "    \"<s>\",\n",
    "    \"<pad>\",\n",
    "    \"</s>\",\n",
    "    \"<unk>\",\n",
    "    \"<mask>\",\n",
    "])\n",
    "\n",
    "token_dir = 'bpe'\n",
    "if not os.path.exists(token_dir):\n",
    "    os.makedirs(token_dir)\n",
    "\n",
    "tokenizer.save_model('bpe')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab76dd6c",
   "metadata": {},
   "source": [
    "## **Loading the vocab file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db2fbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_bpe():\n",
    "    with open(\"bpe/vocab.json\") as f:\n",
    "        vocab = json.load(f)\n",
    "#     with open(\"bpe/merges.txt\") as f:\n",
    "#         merges = f.read()\n",
    "        \n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        \"bpe/vocab.json\",\n",
    "        \"bpe/merges.txt\",\n",
    "    )\n",
    "    \n",
    "    return vocab, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2e2fd6",
   "metadata": {},
   "source": [
    "## **Tokenizing the whole training dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0acd98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "\n",
    "def tokenize(text):\n",
    "    start = 0\n",
    "    end = int(100e6)\n",
    "    offset = int(100e6)\n",
    "    total_length = len(text)\n",
    "    bpe = []\n",
    "    \n",
    "    vocab, tokenizer = load_bpe()\n",
    "\n",
    "    while start != end:\n",
    "        batch = text[start:end]\n",
    "        pattern = re.compile(r'(.*)[\\.?!\\n]', re.DOTALL)\n",
    "        match = pattern.search(batch)\n",
    "\n",
    "        if match:\n",
    "            extracted_text = match.group()\n",
    "        else:\n",
    "            extracted_text = ''\n",
    "\n",
    "        end = end - (len(batch) - len(extracted_text))\n",
    "\n",
    "        splits = tokenizer.encode(extracted_text).tokens\n",
    "        bpe.append(splits)\n",
    "\n",
    "        start = end\n",
    "        end = end+offset if end+offset <= total_length else total_length\n",
    "    \n",
    "    bpe = sum(bpe, [])\n",
    "    encode = lambda x: [vocab[_] for _ in x]\n",
    "    input_ids = torch.tensor(encode(bpe))\n",
    "    \n",
    "    return input_ids, len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ac7760",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_data()\n",
    "input_ids, vocab_size = tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c48c16",
   "metadata": {},
   "source": [
    "## **Storing Tokenized Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf8048a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = {\n",
    "    'input_ids': input_ids,\n",
    "    'vocab_size': vocab_size\n",
    "}\n",
    "\n",
    "# torch.save(tokenized_data, 'bpe/tokenized_data.pt')\n",
    "torch.save(tokenized_data, '<enter_path>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d109595",
   "metadata": {},
   "source": [
    "## **Loading Tokenized Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b72bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized():\n",
    "    tokenized_data = torch.load(\"<enter_path>\")\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    vocab_size = tokenized_data['vocab_size']\n",
    "    \n",
    "    return input_ids, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a57097",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, vocab_size = load_tokenized()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
