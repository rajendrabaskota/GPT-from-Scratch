{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building a word level language model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:07:10.359536Z",
     "iopub.status.busy": "2023-06-18T07:07:10.358987Z",
     "iopub.status.idle": "2023-06-18T07:07:12.640826Z",
     "shell.execute_reply": "2023-06-18T07:07:12.639441Z",
     "shell.execute_reply.started": "2023-06-18T07:07:10.359434Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m F\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import os \n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:07:46.818198Z",
     "iopub.status.busy": "2023-06-18T07:07:46.817494Z",
     "iopub.status.idle": "2023-06-18T07:07:46.827447Z",
     "shell.execute_reply": "2023-06-18T07:07:46.826033Z",
     "shell.execute_reply.started": "2023-06-18T07:07:46.818153Z"
    }
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS\n",
    "\n",
    "batch_size = 128\n",
    "block_size = 256 # window length\n",
    "n_embd = 384 # dimension of embedding vector\n",
    "n_head = 6 # number of heads in multi_head attention\n",
    "n_layers = 6 # number of decoder layers Nx\n",
    "dropout_rate = 0.3\n",
    "eval_iters = 200 # take average of eval_iters output while evaluating\n",
    "eval_interval = 500 # evaluate every eval_interval\n",
    "learning_rate = 3e-4\n",
    "train_ratio = 0.9 # train-test-split\n",
    "\n",
    "prev_model = False\n",
    "if prev_model:\n",
    "    checkpoint_path = \"/kaggle/working/checkpoint-2.pth.tar\"\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-18T07:13:45.191198Z",
     "iopub.status.busy": "2023-06-18T07:13:45.190699Z",
     "iopub.status.idle": "2023-06-18T07:13:45.212674Z",
     "shell.execute_reply": "2023-06-18T07:13:45.211105Z",
     "shell.execute_reply.started": "2023-06-18T07:13:45.191164Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['jataka-tales-volume-5.epub.txt', 'diligent-ambassadors.epub.txt', 'holiday-romance-short-stories-book-two.epub.txt', 'reclaiming-your-soul-working-with-the-mystery.epub.txt', 'high-council-files-nickys-tale.epub.txt', 'hearts-in-darkness.epub.txt']\n"
     ]
    }
   ],
   "source": [
    "# books = os.listdir(\"/kaggle/input/bookcropus/books1/epubtxt\")[:6]\n",
    "# print(books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:48.187630Z",
     "iopub.status.busy": "2023-06-17T20:16:48.186886Z",
     "iopub.status.idle": "2023-06-17T20:16:48.207752Z",
     "shell.execute_reply": "2023-06-17T20:16:48.206719Z",
     "shell.execute_reply.started": "2023-06-17T20:16:48.187591Z"
    }
   },
   "outputs": [],
   "source": [
    "# text = \"\"\n",
    "# for book in books:\n",
    "#     with open(\"/kaggle/input/bookcropus/books1/epubtxt/\"+book, 'r', encoding='utf-8') as f:\n",
    "#         a_book = f.read()\n",
    "#         a_book = a_book.lower()\n",
    "#     text += a_book\n",
    "\n",
    "text = \"\"\n",
    "with open(\"datasets/tiny-shakespeare.txt\", 'r', encoding='utf-8') as f:\n",
    "    a_book = f.read()\n",
    "    a_book = a_book.lower()\n",
    "text += a_book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:48.367899Z",
     "iopub.status.busy": "2023-06-17T20:16:48.367461Z",
     "iopub.status.idle": "2023-06-17T20:16:48.396212Z",
     "shell.execute_reply": "2023-06-17T20:16:48.394952Z",
     "shell.execute_reply.started": "2023-06-17T20:16:48.367864Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1115394\n",
      "Number of words: 202651\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of characters: {len(text)}\")\n",
    "print(f\"Number of words: {len(text.split())}\") #number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:48.579890Z",
     "iopub.status.busy": "2023-06-17T20:16:48.579480Z",
     "iopub.status.idle": "2023-06-17T20:16:48.826061Z",
     "shell.execute_reply": "2023-06-17T20:16:48.824831Z",
     "shell.execute_reply.started": "2023-06-17T20:16:48.579854Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 11473\n"
     ]
    }
   ],
   "source": [
    "vocab_list = re.findall(r'\\w+|[^\\w\\S]+|[^\\w]', text) # tokenizing\n",
    "vocab_list = sorted(list(set(vocab_list)))\n",
    "vocab_size = len(vocab_list)\n",
    "print(f\"Vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:48.829622Z",
     "iopub.status.busy": "2023-06-17T20:16:48.828269Z",
     "iopub.status.idle": "2023-06-17T20:16:48.842179Z",
     "shell.execute_reply": "2023-06-17T20:16:48.840932Z",
     "shell.execute_reply.started": "2023-06-17T20:16:48.829574Z"
    }
   },
   "outputs": [],
   "source": [
    "stoi = {value: key for key, value in enumerate(vocab_list)}\n",
    "itos = {key: value for value, key in stoi.items()}\n",
    "encode = lambda x: [stoi[_] for _ in x]\n",
    "decode = lambda x: ''.join(itos[_] for _ in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:49.039612Z",
     "iopub.status.busy": "2023-06-17T20:16:49.039203Z",
     "iopub.status.idle": "2023-06-17T20:16:49.368473Z",
     "shell.execute_reply": "2023-06-17T20:16:49.367336Z",
     "shell.execute_reply.started": "2023-06-17T20:16:49.039576Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mtensor(encode(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mS]+|[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw]\u001b[39m\u001b[38;5;124m'\u001b[39m, text)))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[:\u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(re.findall(r'\\w+|[^\\w\\S]+|[^\\w]', text)))\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train-Test split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:49.456965Z",
     "iopub.status.busy": "2023-06-17T20:16:49.456533Z",
     "iopub.status.idle": "2023-06-17T20:16:49.463818Z",
     "shell.execute_reply": "2023-06-17T20:16:49.462481Z",
     "shell.execute_reply.started": "2023-06-17T20:16:49.456930Z"
    }
   },
   "outputs": [],
   "source": [
    "n = int(train_ratio * len(data))\n",
    "x_train = data[:n]\n",
    "x_test = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:49.673406Z",
     "iopub.status.busy": "2023-06-17T20:16:49.672655Z",
     "iopub.status.idle": "2023-06-17T20:16:49.681025Z",
     "shell.execute_reply": "2023-06-17T20:16:49.679473Z",
     "shell.execute_reply.started": "2023-06-17T20:16:49.673368Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x):\n",
    "        super(CustomDataset, self).__init__()\n",
    "        self.x = x\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.x.shape[0] - block_size\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx:idx+block_size], self.x[idx+1:idx+1+block_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:49.883381Z",
     "iopub.status.busy": "2023-06-17T20:16:49.882620Z",
     "iopub.status.idle": "2023-06-17T20:16:49.889162Z",
     "shell.execute_reply": "2023-06-17T20:16:49.887827Z",
     "shell.execute_reply.started": "2023-06-17T20:16:49.883344Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dataloader(dataset, batch_size):\n",
    "    dataloader = torch.utils.data.DataLoader(dataset=CustomDataset(dataset),\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True,\n",
    "                                            pin_memory=False)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Self-Attention**\n",
    "* ### Many ways to implement self-attention.\n",
    "* ### Masking needs to be implemented which generally means we don't look at the future tokens for prediction (just look at the past         tokens)\n",
    "* ### The simplest idea to determine the affinity/attention is to take an average of all the past tokens in each channel.\n",
    "* ### The best way is to use the softmax as mentioned in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:50.313914Z",
     "iopub.status.busy": "2023-06-17T20:16:50.312992Z",
     "iopub.status.idle": "2023-06-17T20:16:50.327207Z",
     "shell.execute_reply": "2023-06-17T20:16:50.325902Z",
     "shell.execute_reply.started": "2023-06-17T20:16:50.313874Z"
    }
   },
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    # ONE HEAD OF SELF-ATTENTION\n",
    "    \n",
    "    def __init__(self, head_size):\n",
    "        # The head size is denoted by dk in the paper\n",
    "        # The dimension of head size is generally equal to the dimension of the embedding vector\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones((block_size, block_size))))\n",
    "                             # Registers a buffer that is not considered a model parameter\n",
    "                            # Here tril isn't a model parameter to learn. so we register it as a buffer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape # C is equal to the head size\n",
    "        k = self.key(x) # gives output (B, T, C)\n",
    "        q = self.query(x) # output of (B, T, C)\n",
    "        v = self.value(x) # output of (B, T, C)\n",
    "        wei = q @ k.transpose(-2, -1) # (B, T, C) @ (B, C, T) --> (B, T, T) this (B, T, T) represents the amount of affinity each token has\n",
    "                                        # with other tokens defined inside that block_size(window)\n",
    "        wei = wei / C**0.5 # normalizing the weights; controls the variance; for more explanation look in the rough section\n",
    "        tril = torch.tril(torch.ones((T, T), device=device))\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, -1)\n",
    "        wei = self.dropout(wei)\n",
    "        out = wei @ v\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Multi-Head Attention**\n",
    "\n",
    "* ### The idea is to simply concatenate number of self-     attention heads.\n",
    "* ### The output dimension after concatenation is equal to   n_embd(or the input to the multi-head attention layer).\n",
    "* ### There is also a projection layer(a linear layer) after multi-head attention layer. The projection layer is required as we implement skip-connections. because the output of the multi-head attention adds up to the residual pathway. So first we perform projection and then add the output to the residual pathway. (mentioned in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:50.932115Z",
     "iopub.status.busy": "2023-06-17T20:16:50.929911Z",
     "iopub.status.idle": "2023-06-17T20:16:50.941082Z",
     "shell.execute_reply": "2023-06-17T20:16:50.939712Z",
     "shell.execute_reply.started": "2023-06-17T20:16:50.932063Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.projection = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.projection(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feed Forward Network**\n",
    "\n",
    "* ### The implementation of feed forward in transformer is on the per token basis(i.e. all the tokens do this independently.\n",
    "* ### The self-attention is the communication between the tokens and once they have gathered all the data they have to think on the data          independently which is done by FFN.\n",
    "* ### FFN, too, contains the projection layer before adding up to the residual pathway.\n",
    "* ### \" This consists of two linear transformations with a ReLU activation in between.\" (as mentioned in the paper)\n",
    "* ### Note that the dimension of the inner layer is multiplied by 4 as mentioned in the paper. \"The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048.\" (2048/512=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:51.436954Z",
     "iopub.status.busy": "2023-06-17T20:16:51.436522Z",
     "iopub.status.idle": "2023-06-17T20:16:51.444247Z",
     "shell.execute_reply": "2023-06-17T20:16:51.443170Z",
     "shell.execute_reply.started": "2023-06-17T20:16:51.436909Z"
    }
   },
   "outputs": [],
   "source": [
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4*n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4*n_embd, n_embd), # the projection layer\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementation of block**\n",
    "* ### We are implementing decoder block in this project to be more specific.\n",
    "* ### There are *Nx* number of blocks as mentioned in the paper.\n",
    "* ### Intersperse communication with computation(meaning a block contains attention(communication) and FFN(computation)) and replicates the   same for *Nx* number of times.\n",
    "* ### Skip-connection implemented for both attention sub-layer and FFN sub-layer.\n",
    "* ### There is also the implementation of layer norm which is applied before feeding the inputs into the sub-layer blocks. The original   paper performs layer norm to the output of the sub-layers.\n",
    "* ### Also added dropout for both sub-layers before adding to the residual pathway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:51.869206Z",
     "iopub.status.busy": "2023-06-17T20:16:51.867932Z",
     "iopub.status.idle": "2023-06-17T20:16:51.879071Z",
     "shell.execute_reply": "2023-06-17T20:16:51.877778Z",
     "shell.execute_reply.started": "2023-06-17T20:16:51.869153Z"
    }
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_head gives the number of heads for multi-head attention\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head # gives the output dimension of one head\n",
    "        self.sa_head = MultiHeadAttention(num_heads=n_head, head_size=head_size) # since we concat at the end in multihead attention,\n",
    "                                                                            # the head size for one attention head = n_embd / num_heads\n",
    "                                                                            # kind of like a group conv in CNN, instead of a large filter we\n",
    "        self.FFN = FeedForwardNetwork(n_embd=n_embd)\n",
    "        self.layer_norm1 = nn.LayerNorm(n_embd)\n",
    "        self.layer_norm2 = nn.LayerNorm(n_embd)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.sa_head(self.layer_norm1(x)) # implementing skip connection/skip connection\n",
    "        x = x + self.FFN(self.layer_norm2(x)) # implementing skip connection/skip connection\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:52.398476Z",
     "iopub.status.busy": "2023-06-17T20:16:52.397944Z",
     "iopub.status.idle": "2023-06-17T20:16:52.416243Z",
     "shell.execute_reply": "2023-06-17T20:16:52.414992Z",
     "shell.execute_reply.started": "2023-06-17T20:16:52.398433Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPTLanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head) for _ in range(n_layers)])\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(n_embd) # the final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "                \n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, idx, target=None):\n",
    "        B, T = idx.shape\n",
    "        \n",
    "        tok_emb = self.token_embedding_table(idx) #token embeddings, gives output of shape (B, T, C) here C = n_embd = 32\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # gives output of shape (T, C)\n",
    "        x = tok_emb + pos_emb # (B, T, C) + (T, C) --> (B, T, C)\n",
    "        x = self.blocks(x)\n",
    "        x = self.layer_norm(x)\n",
    "        logits = self.lm_head(x) # output of shape (B, T, vocab_size)\n",
    "        \n",
    "        \n",
    "        # Explanation of B, T, C\n",
    "        # B- batch dimension\n",
    "        # T - time dimension (timestep) in this project one character=one timestep\n",
    "        # C - channel dimension ie, the dimension of the embedding vector\n",
    "        if target is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C) # logits of shape B*T, C\n",
    "#             print(f\"Before: {target}\")\n",
    "            target = target.view(B*T)\n",
    "#             print(f\"After: {target}, its shape is {target.shape}\")\n",
    "            loss = F.cross_entropy(logits, target)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens): # generates output given idx ie, the starting token\n",
    "        # idx is (B, T) array of indices\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_trunc = idx[:, -block_size:] # making sure to truncate tokens if we receive more number of tokens than block_size\n",
    "                                             # just to make sure position embedding doesn't throw any error\n",
    "            logits, loss = self(idx_trunc) # logits of shape B, T, C\n",
    "#             print(f\"logits shape: {logits.shape}\")\n",
    "#             print(f\"logits: {logits}\")\n",
    "            logits = logits[:, -1, :] #logits of shape B, C\n",
    "#             print(f\"logits after filter shape: {logits.shape}\")\n",
    "#             print(f\"logits after filter: {logits}\")\n",
    "            probs = F.softmax(logits, dim=-1) # probs of shape\n",
    "#             print(f\"probs shape: {probs.shape}\")\n",
    "#             print(f\"probs: {probs}\")\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "#             print(f\"next index shape: {idx_next.shape}\")\n",
    "#             print(f\"next index: {idx_next}\")\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "#             print(f\"index shape: {idx.shape}\")\n",
    "#             print(f\"index tensor: {idx}\")\n",
    "        return idx\n",
    "\n",
    "# we can also predict the loss:\n",
    "# -ln(1/65) = 4.17438727, where 65 is the vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:52.998645Z",
     "iopub.status.busy": "2023-06-17T20:16:52.997931Z",
     "iopub.status.idle": "2023-06-17T20:16:53.007054Z",
     "shell.execute_reply": "2023-06-17T20:16:53.005980Z",
     "shell.execute_reply.started": "2023-06-17T20:16:52.998608Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, train_dl, test_dl):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            if split == 'train':\n",
    "                X, y = next(iter(train_dl))\n",
    "            elif split == 'test':\n",
    "                X, y = next(iter(test_dl))\n",
    "            logits, loss = model(X.to(device), y.to(device))\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:53.471986Z",
     "iopub.status.busy": "2023-06-17T20:16:53.471556Z",
     "iopub.status.idle": "2023-06-17T20:16:53.478140Z",
     "shell.execute_reply": "2023-06-17T20:16:53.477115Z",
     "shell.execute_reply.started": "2023-06-17T20:16:53.471944Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, train_loss, test_loss, filename):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'train_loss': train_loss,\n",
    "        'test_loss': test_loss,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict()\n",
    "    }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:55.349473Z",
     "iopub.status.busy": "2023-06-17T20:16:55.348682Z",
     "iopub.status.idle": "2023-06-17T20:16:55.364042Z",
     "shell.execute_reply": "2023-06-17T20:16:55.362810Z",
     "shell.execute_reply.started": "2023-06-17T20:16:55.349436Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, train_dl, test_dl):\n",
    "    loss_per_epoch = {'train': [], 'test': []}\n",
    "    if prev_model:\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        epoch_offset = checkpoint['epoch']\n",
    "    else:\n",
    "        epoch_offset = 0\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        if prev_model and epoch == 0:\n",
    "            losses = {\n",
    "                'train': checkpoint['train_loss'],\n",
    "                'test': checkpoint['test_loss']\n",
    "            }\n",
    "        else:\n",
    "            losses = estimate_loss(model, train_dl, test_dl)\n",
    "            loss_per_epoch['train'].append(losses['train'])\n",
    "            loss_per_epoch['test'].append(losses['test'])\n",
    "            save_checkpoint(model=model, optimizer=optimizer, epoch=epoch_offset+epoch, train_loss=losses['train'], test_loss=losses['test'], filename=\"/kaggle/working/checkpoint-\"+str(epoch_offset+epoch)+\".pth.tar\")\n",
    "\n",
    "        print(f\"Step {epoch_offset+epoch}: train loss {losses['train']:.4f}, Test loss {losses['test']:.4f}\")\n",
    "            \n",
    "        for data, target in tqdm(train_dl):\n",
    "            logits, loss = model(data.to(device), target.to(device))\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        print(decode(model.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=3000)[0].tolist()))        \n",
    "            \n",
    "    return loss_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:57.123004Z",
     "iopub.status.busy": "2023-06-17T20:16:57.122456Z",
     "iopub.status.idle": "2023-06-17T20:16:57.136938Z",
     "shell.execute_reply": "2023-06-17T20:16:57.135337Z",
     "shell.execute_reply.started": "2023-06-17T20:16:57.122957Z"
    }
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    train_dl = prepare_dataloader(x_train, batch_size)\n",
    "    test_dl = prepare_dataloader(x_test, batch_size)\n",
    "    model = GPTLanguageModel().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    print(f\"Total number of parameters: {sum(p.numel() for p in model.parameters())/1e6} M\")\n",
    "    \n",
    "    epochs = 5\n",
    "    losses = train(model, optimizer, epochs, train_dl, test_dl)\n",
    "    plot_graph(losses)\n",
    "    print(decode(model.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=3000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:16:57.751530Z",
     "iopub.status.busy": "2023-06-17T20:16:57.751122Z",
     "iopub.status.idle": "2023-06-17T20:16:57.758989Z",
     "shell.execute_reply": "2023-06-17T20:16:57.757657Z",
     "shell.execute_reply.started": "2023-06-17T20:16:57.751494Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_graph(losses):\n",
    "    train_loss = losses['train']\n",
    "    test_loss = losses['test']\n",
    "    epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "    plt.plot(epochs, train_loss, 'b', label='Train Loss')\n",
    "    plt.plot(epochs, test_loss, 'r', label='Test Loss')\n",
    "    plt.title('Loss vs Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-06-17T20:17:03.664391Z",
     "iopub.status.busy": "2023-06-17T20:17:03.663968Z",
     "iopub.status.idle": "2023-06-17T23:23:07.352893Z",
     "shell.execute_reply": "2023-06-17T23:23:07.351598Z",
     "shell.execute_reply.started": "2023-06-17T20:17:03.664357Z"
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(decode(model.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=3000)[0].tolist()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
